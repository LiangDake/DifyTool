from email.message import EmailMessage

from langchain.chains.combine_documents import create_stuff_documents_chain
from langchain_community.chains.graph_qa.cypher import GraphCypherQAChain
from langchain_community.graphs import Neo4jGraph
from langchain_experimental.graph_transformers import LLMGraphTransformer
from py2neo import Graph
import file_processing
import utils
import os
import shutil
from langchain_community.embeddings.ollama import OllamaEmbeddings
from langchain_community.vectorstores import Chroma
from langchain_chroma import Chroma
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain.schema.document import Document
from langchain_community.llms.ollama import Ollama
from langchain_core.prompts import ChatPromptTemplate, PromptTemplate


# çŸ¥è¯†åº“ç±»
class KnowledgeBase:
    def __init__(self, chroma_path="default"):
        # é»˜è®¤ç¼–ç æ¨¡å‹
        self.embeddings = OllamaEmbeddings(model="mxbai-embed-large")
        # é»˜è®¤çŸ¥è¯†åº“
        self.chroma_path = chroma_path
        if self.chroma_path == "default":
            self.db = Chroma(
                embedding_function=self.embeddings
            )
        else:
            self.db = Chroma(
                persist_directory=self.chroma_path, embedding_function=self.embeddings
            )

    # æ–‡å­—åˆ†ç¦»
    def split_documents(self, documents: list[Document]):
        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=800,
            chunk_overlap=80,
            length_function=len,
            is_separator_regex=False,
        )
        return text_splitter.split_documents(documents)

    # è®¡ç®—å¹¶è®°å½•æ–‡ä»¶id
    def calculate_chunk_ids(self, chunks):

        # This will create IDs like "data/monopoly.pdf:6:2"
        # Page Source : Page Number : Chunk Index

        last_page_id = None
        current_chunk_index = 0

        for chunk in chunks:
            source = chunk.metadata.get("source")
            page = chunk.metadata.get("page")
            current_page_id = f"{source}:{page}"

            # If the page ID is the same as the last one, increment the index.
            if current_page_id == last_page_id:
                current_chunk_index += 1
            else:
                current_chunk_index = 0

            # Calculate the chunk ID.
            chunk_id = f"{current_page_id}:{current_chunk_index}"
            last_page_id = current_page_id

            # Add it to the page meta-data.
            chunk.metadata["id"] = chunk_id

        return chunks

    # æ·»åŠ æ–‡ä»¶è‡³chroma
    def add_to_chroma(self, chunks: list[Document]):
        # Load the existing database.

        # Calculate Page IDs.
        chunks_with_ids = self.calculate_chunk_ids(chunks)

        # Add or Update the documents.
        existing_items = self.db.get(include=[])  # IDs are always included by default
        existing_ids = set(existing_items["ids"])
        print(f"Number of existing documents in DB: {len(existing_ids)}")

        # Only add documents that don't exist in the DB.
        new_chunks = []
        for chunk in chunks_with_ids:
            if chunk.metadata["id"] not in existing_ids:
                new_chunks.append(chunk)

        if len(new_chunks):
            print(f"ğŸ‘‰ Adding new documents: {len(new_chunks)}")
            new_chunk_ids = [chunk.metadata["id"] for chunk in new_chunks]
            self.db.add_documents(new_chunks, ids=new_chunk_ids)
            self.db.persist()
        else:
            print("âœ… No new documents to add")

    # å®ç°å¤šä¸ªæ–‡ä»¶çš„å¯¼å…¥å¹¶å­˜å‚¨åœ¨çŸ¥è¯†åº“ä¸­
    def add_folder(self, folder_path: str) -> str:
        for filename in os.listdir(folder_path):
            # æ„å»ºå®Œæ•´çš„æ–‡ä»¶è·¯å¾„
            if filename.startswith('.'):
                continue
            file_path = os.path.join(folder_path, filename)
            doc = file_processing.import_file(file_path)
            if doc is not False:
                chunks = self.split_documents(doc)
                # åµŒå…¥å¹¶å­˜å…¥Chromaæ•°æ®åº“
                self.add_to_chroma(chunks)
            else:
                print(f"{file_path}æ–‡ä»¶ç±»å‹é”™è¯¯æˆ–å¯¼å…¥å¤±è´¥")
        return "æ‰€æœ‰æ–‡ä»¶å·²å…¨éƒ¨å¯¼å…¥çŸ¥è¯†åº“ï¼Œè¯·æ£€æŸ¥æ–‡ä»¶å†…å®¹"

    def delete_from_chroma(self, file_path):
        pass

    # æ¸…é™¤çŸ¥è¯†åº“å†…å®¹
    def clear_database(self):
        if os.path.exists(self.chroma_path):
            shutil.rmtree(self.chroma_path)

    # Emailé¢„å¤„ç†


# æ£€ç´¢çŸ¥è¯†åº“ç±»ï¼Œå…¶ä¸­åŒ…æ‹¬ï¼šï¼ˆç›¸å…³äººç‰©çš„ï¼‰æŸ¥è¯¢ã€çº¿ç´¢å‘ç°ã€åˆ†æã€æ€»ç»“ã€å†™ä½œ
# è®°å¾—æ·»åŠ è¯­éŸ³åˆ†æåŠŸèƒ½â€¼ï¸
class QueryKnowledgeBase():
    def __init__(self, chroma_path="image", file_num=5):
        # é»˜è®¤ç¼–ç æ¨¡å‹
        self.embeddings = OllamaEmbeddings(model="yxl/m3e")
        # é»˜è®¤çŸ¥è¯†åº“
        self.chroma_path = chroma_path
        if self.chroma_path == "default":
            self.db = Chroma(
                embedding_function=self.embeddings
            )
        else:
            self.db = Chroma(
                persist_directory=self.chroma_path, embedding_function=self.embeddings
            )
        # é»˜è®¤æ¨¡å‹ä¸ºqwen2:7b
        self.llm = Ollama(model="qwen2")
        # é»˜è®¤æ£€ç´¢ç‰‡æ®µæ•°
        self.file_num = file_num
        self.PROMPT_TEMPLATE = """
        æ ¹æ®ä»¥ä¸‹contextå›ç­”é—®é¢˜ï¼š{question}ï¼Œè¦æ±‚è¯¦ç»†ä½¿ç”¨å¹¶ä¸”è¯¦ç»†æŒ‡å‡ºçŸ¥è¯†æ¥æºã€‚
        {context}
        """

    def query_context(self, query_text: str) -> str:
        # æ ¹æ®æŸ¥è¯¢è·å–ç›¸å…³å†…å®¹
        # æœç´¢å‘é‡æ•°æ®åº“
        results = self.db.similarity_search_with_score(query_text, k=self.file_num)
        print(results)
        context_text = "\n\n---\n\n".join([doc.page_content for doc, _score in results])
        # # è¾“å‡ºæŸ¥æ‰¾åˆ°çš„å†…å®¹
        # print(context_text)
        # print("-----------------------")
        return context_text

    def query_content_with_sources(self, query_text: str) -> str:
        # æ ¹æ®æŸ¥è¯¢è·å–ç›¸å…³å†…å®¹
        # æœç´¢å‘é‡æ•°æ®åº“
        results = self.db.similarity_search_with_score(query_text, k=self.file_num)
        # print(results)
        context_text = "\n\n---\n\n".join([doc.page_content for doc, _score in results])
        # # è¾“å‡ºæŸ¥æ‰¾åˆ°çš„å†…å®¹
        # print(context_text)
        # print("-----------------------")
        prompt_template = ChatPromptTemplate.from_template(self.PROMPT_TEMPLATE)
        prompt = prompt_template.format(context=context_text, question=query_text)
        # model = Ollama(model="qwen2")
        response_text = self.llm.invoke(prompt)

        sources = [doc.metadata.get("id", None) for doc, _score in results]
        formatted_response = f"{response_text}\n\næ–‡ä»¶æ¥æºå¦‚ä¸‹: {sources}"
        # print(formatted_response)
        return formatted_response

    def query_content(self, query_text: str) -> str:
        # æ ¹æ®æŸ¥è¯¢è·å–ç›¸å…³å†…å®¹
        # æœç´¢å‘é‡æ•°æ®åº“
        results = self.db.similarity_search_with_score(query_text, k=self.file_num)
        context_text = "\n\n---\n\n".join([doc.page_content for doc, _score in results])

        prompt_template = ChatPromptTemplate.from_template(self.PROMPT_TEMPLATE)
        prompt = prompt_template.format(context=context_text, question=query_text)

        response_text = self.llm.invoke(prompt)
        return response_text


# æ–‡ä»¶ç±»ï¼Œå…¶ä¸­åŒ…æ‹¬ï¼šï¼ˆå¤šæ–‡ä»¶ï¼‰æ€»ç»“ä¸åˆ†æã€ï¼ˆä¿ç•™æ ¼å¼çš„é«˜ç²¾åº¦ï¼‰ç¿»è¯‘ã€ï¼ˆé«˜è´¨é‡ã€ç»Ÿä¸€æ ¼å¼ï¼‰å†™ä½œ
class QueryFileBase(QueryKnowledgeBase):
    def __init__(self):
        super().__init__()

    def translate_file(self, file_path: str):
        # å®ç°ç¿»è¯‘é€»è¾‘
        # è·å–æ–‡ä»¶ä¸­çš„æ–‡å­—
        doc = file_processing.import_file(file_path)
        # ç¡®ä¿æ ¼å¼æ­£ç¡®
        if doc is not False:
            source_text = doc[0].page_content
            translation = utils.translate(
                source_text=source_text
            )
            # è·å–æ–‡ä»¶ç±»å‹
            file_type = file_processing.get_file_type(file_path)
            if file_type in '.pdf':
                # å¦‚æœæ˜¯ PDF æ–‡ä»¶ï¼Œè°ƒç”¨ save_translated_pdf ä¿å­˜
                translated_file_path = file_processing.save_translated_pdf(file_path, translation)
            else:
                # å¦åˆ™é»˜è®¤ä¿å­˜ä¸º DOCX
                translated_file_path = file_processing.save_translated_docx(file_path, translation)

            return translated_file_path

    def translate_email(self, file_path: str):
        # å¯¼å…¥é‚®ä»¶è·å¾—ç»“æœ
        result = file_processing.import_email(file_path, "email")
        # é‚®ä»¶æ­£æ–‡
        doc = result["doc"]
        # é‚®ä»¶å¤´
        email_headers = result["email_headers"]
        # é‚®ä»¶é™„ä»¶æ–‡ä»¶å¤¹
        attachment_folder = result["output_folder"]
        # ç¿»è¯‘åçš„é™„ä»¶æ–‡ä»¶å¤¹
        translated_attachments = []

        # å¯¹é‚®ä»¶æ­£æ–‡è¿›è¡Œç¿»è¯‘
        if doc is not False:
            source_text = doc[0].page_content
            translation = utils.translate(
                source_text=source_text
            )
        else:
            # æ— æ­£æ–‡
            translation = None

        # æ£€æŸ¥é™„ä»¶æ–‡ä»¶å¤¹æ˜¯å¦å­˜åœ¨æ–‡ä»¶
        if os.path.exists(attachment_folder) and len(os.listdir(attachment_folder)) > 0:
            # è·å–é™„ä»¶æ–‡ä»¶åˆ—è¡¨
            attachments = os.listdir(attachment_folder)
            # éå†é™„ä»¶å¹¶ç¿»è¯‘
            for file_name in attachments:
                attachment_file_path = os.path.join(attachment_folder, file_name)
                translated_file_path = self.translate_file(attachment_file_path)
                translated_attachments.append(translated_file_path)

        # é‡æ–°åˆ›å»ºæ–°çš„EML
        translated_file_path = file_processing.save_translated_email(
            file_path,
            email_headers,
            translation,
            translated_attachments
        )
        # æ¸…ç©ºé™„ä»¶æ–‡ä»¶å¤¹
        file_processing.delete_folder(attachment_folder)
        print(f"{file_path}å·²æˆåŠŸç¿»è¯‘ï¼\n")

        return translated_file_path


    def conclusion(self, folder_path: str):
        conclusion_prompt = ChatPromptTemplate.from_messages(
            [("system", "ç®€è¦æ¦‚æ‹¬ä»¥ä¸‹å†…å®¹ï¼Œè¦æ±‚å†…å®¹æ–‡å­—è¿è´¯ï¼Œå¹¶æ‰¾åˆ°å…¶ä¸­çš„å…³è”æ€§:\\n\\n{context}ã€‚ä»¥ä¸­æ–‡è¾“å‡ºä½ æ¦‚æ‹¬çš„å†…å®¹ã€‚")]
        )
        context = []
        for file in os.listdir(folder_path):
            file_path = os.path.join(folder_path, file)
            docs = file_processing.import_file(file_path)
            context.append(docs[0])

        chain = create_stuff_documents_chain(self.llm, conclusion_prompt)
        # Invoke chain
        result = chain.invoke({"context": context})
        return result


# å›¾è°±ç±»
class GraphBase(QueryFileBase):
    def __init__(self):
        super().__init__()
        os.environ["NEO4J_URI"] = "bolt://localhost:7687"
        os.environ["NEO4J_USERNAME"] = "neo4j"
        os.environ["NEO4J_PASSWORD"] = "lkzxxzcsc2020"
        os.environ["NEO4J_DATABASE"] = "neo4j"

        self.graph = Neo4jGraph(url="bolt://localhost:7687", username="neo4j", password="lkzxxzcsc2020")

        # è¿æ¥åˆ°Neo4jæ•°æ®åº“
        self.graph_db = Graph(os.getenv("NEO4J_URI"), auth=(os.getenv("NEO4J_USERNAME"), os.getenv("NEO4J_PASSWORD")))

    # ç®€æ˜“ç¿»è¯‘
    def simple_translate(self, content):
        translate_template = "å°†ä»¥ä¸‹æ–‡æœ¬ç¿»è¯‘ä¸ºä¸­æ–‡: {text}"
        translate_prompt = PromptTemplate.from_template(translate_template)
        llm = self.llm
        chain = (
                translate_prompt | llm
        )
        result = chain.invoke({"text": content})
        return result

    def build_knowledge_graph(self, file_path: str):
        doc = file_processing.import_file(file_path)
        llm_transformer = LLMGraphTransformer(
            llm=self.llm
        )
        graph_documents = llm_transformer.convert_to_graph_documents(documents=doc)
        print(f"Nodes:{graph_documents[0].nodes}")
        print(f"Relationships:{graph_documents[0].relationships}")

        try:
            self.graph.add_graph_documents(graph_documents)
            return True
        except Exception:
            return False

    def build_graph(self, folder_path: str):
        text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=20)
        for file in os.listdir(folder_path):
            file_path = os.path.join(folder_path, file)
            docs = file_processing.import_file(file_path)
            # æ–‡ä»¶å¯¼å…¥æˆåŠŸ
            if docs:
                print(f"{file} å·²æˆåŠŸè¯»å–\n")
                # # åˆ†å‰²æ–‡æ¡£ï¼Œé‚®ä»¶å…¶å®ä¸éœ€è¦åˆ†å‰²ä¹Ÿè¡Œ
                # documents = text_splitter.split_documents(documents=docs)
                # è·å¾—ç¿»è¯‘åçš„æ–‡å­—
                translated_content = self.simple_translate(docs[0].page_content)
                # è·å¾—ç¿»è¯‘åçš„æ–‡æ¡£
                translated_docs = [Document(page_content=translated_content)]
                llm_transformer = LLMGraphTransformer(
                    llm=self.llm
                )
                graph_documents = llm_transformer.convert_to_graph_documents(documents=translated_docs)
                # print(f"Nodes:{graph_documents[0].nodes}")
                # print(f"Relationships:{graph_documents[0].relationships}")
                try:
                    self.graph.add_graph_documents(graph_documents)
                    print(f"{file} å·²æˆåŠŸå¯¼å…¥çŸ¥è¯†å›¾è°±\n\n")
                except Exception:
                    print(f"{file} æœªèƒ½å¯¼å…¥çŸ¥è¯†å›¾è°±\n\n")
                    continue
            else:
                print(f"{file} å¯¼å…¥å¤±è´¥ï¼Œè¯·æ£€æŸ¥æ–‡ä»¶æ ¼å¼\n")
                continue

        print(f"{folder_path} æ–‡ä»¶å¤¹æ‰€æœ‰æ–‡ä»¶å·²æˆåŠŸå¯¼å…¥çŸ¥è¯†å›¾è°±ã€‚\n")


    def query_knowleage_graph(self, query):
        chain = GraphCypherQAChain.from_llm(
            self.llm,
            graph=self.graph,
            verbose=True
        )
        result = chain.invoke({"query": query})
        final_answer = result['result']
        return final_answer

    def delete_knowleage_graph(self):
        try:
            # æ¸…ç©ºNeo4jæ•°æ®åº“
            self.graph_db.run("MATCH (n) DETACH DELETE n")
            return True
        except Exception:
            return False


# # åˆå§‹åŒ–å›¾çŸ¥è¯†åº“ï¼ˆå›¾å½¢æ•°æ®åº“éœ€è¦æ›´æ”¹URLåœ°å€ä¸è´¦å·å¯†ç ï¼‰
# graph_base = GraphBase()
# # # æ„å»ºçŸ¥è¯†å›¾è°±ï¼ˆå…ˆåˆ†å—å†ç¿»è¯‘å†æ„å»ºï¼‰
# # result = graph_base.build_graph('/Users/liangdake/Downloads/å·¥ä½œ/äººå·¥æ™ºèƒ½å®ä¹ /email')
#
# result2 = graph_base.query_knowleage_graph("Find the information about ç»´å¤šåˆ©äºšå¤§å­¦")
# print(result2)