from icecream import ic
from langchain.chains.combine_documents import create_stuff_documents_chain
from langchain_community.chains.graph_qa.cypher import GraphCypherQAChain
from langchain_community.graphs import Neo4jGraph
from langchain_experimental.graph_transformers import LLMGraphTransformer
from py2neo import Graph

import base
import utils
from langchain_community.document_loaders import *
import os
import shutil
from langchain_community.embeddings.ollama import OllamaEmbeddings
from langchain.vectorstores import Chroma
from langchain_text_splitters import RecursiveCharacterTextSplitter
from paddleocr import PaddleOCR
from langchain.schema.document import Document
from langchain_community.llms.ollama import Ollama
from langchain_core.prompts import ChatPromptTemplate

supported_formats = (
    '.txt', '.pdf', '.docx', '.png', '.jpg',
    ".doc", ".ppt", ".pptx", ".xls", ".xlsx",
    ".htm", ".html", ".eml", ".csv"
)


# å¯¼å…¥æ–‡ä»¶
def import_file(file_path: str):
    # è·å–æ–‡ä»¶ç±»å‹
    _, file_type = os.path.splitext(file_path)
    file_type = file_type.lower()

    if file_type not in supported_formats:
        return False

    elif file_type in '.pdf':
        doc = PyPDFLoader(file_path, extract_images=True).load()

    elif file_type in ('.png', '.jpg'):
        doc = UnstructuredImageLoader(file_path).load()

    elif file_type in (".doc", ".docx"):
        doc = Docx2txtLoader(file_path).load()

    elif file_type in (".ppt", ".pptx"):
        doc = UnstructuredPowerPointLoader(file_path).load()

    elif file_type in (".xls", ".xlsx"):
        doc = UnstructuredExcelLoader(file_path).load()

    elif file_type in (".htm", ".html"):
        doc = BSHTMLLoader(file_path, open_encoding="unicode_escape").load()

    elif file_type in ".eml":
        doc = UnstructuredEmailLoader(file_path=file_path, process_attachments=True).load()

    elif file_type in ".csv":
        doc = CSVLoader(file_path).load()

    elif file_type in ".txt":
        doc = TextLoader(file_path, autodetect_encoding=True).load()

    else:
        doc = TextLoader(file_path, autodetect_encoding=True).load()

    return doc


# ä¿å­˜æ–‡ä»¶
def save_file(file_path: str, source_lang, target_lang):
    # è·å–è¾“å…¥æ–‡ä»¶çš„æ–‡ä»¶åï¼Œä¸å¸¦æ‰©å±•å
    input_file_name = os.path.splitext(os.path.basename(file_path))[0]
    output_folder = '/Users/liangdake/Downloads/å·¥ä½œ/äººå·¥æ™ºèƒ½å®ä¹ /email'  # æ›¿æ¢ä¸ºä½ æƒ³ä¿å­˜ç»“æœçš„æ–‡ä»¶å¤¹è·¯å¾„
    output_file_name = f'{input_file_name}.txt'  # ä½¿ç”¨ç›¸åŒæ–‡ä»¶åï¼Œä½†åç¼€ä¸º.txt
    output_file_path = os.path.join(output_folder, output_file_name)

    result = QueryFileBase().translate_content(file_path, source_lang, target_lang)

    # ç¡®ä¿è¾“å‡ºæ–‡ä»¶å¤¹å­˜åœ¨
    os.makedirs(output_folder, exist_ok=True)

    # å°†ç»“æœå†™å…¥åˆ°æ–°çš„txtæ–‡ä»¶
    with open(output_file_path, 'w', encoding='utf-8') as output_file:
        output_file.write(result)

    print(f"ç»“æœå·²ä¿å­˜åˆ°: {output_file_path}")


# çŸ¥è¯†åº“ç±»
class KnowledgeBase:
    def __init__(self, chroma_path="default"):
        # é»˜è®¤ç¼–ç æ¨¡å‹
        self.embeddings = OllamaEmbeddings(model="mxbai-embed-large")
        # é»˜è®¤çŸ¥è¯†åº“
        self.chroma_path = chroma_path
        if self.chroma_path == "default":
            self.db = Chroma(
                embedding_function=self.embeddings
            )
        else:
            self.db = Chroma(
                persist_directory=self.chroma_path, embedding_function=self.embeddings
            )

    # æ–‡å­—åˆ†ç¦»
    def split_documents(self, documents: list[Document]):
        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=800,
            chunk_overlap=80,
            length_function=len,
            is_separator_regex=False,
        )
        return text_splitter.split_documents(documents)

    # è®¡ç®—å¹¶è®°å½•æ–‡ä»¶id
    def calculate_chunk_ids(self, chunks):

        # This will create IDs like "data/monopoly.pdf:6:2"
        # Page Source : Page Number : Chunk Index

        last_page_id = None
        current_chunk_index = 0

        for chunk in chunks:
            source = chunk.metadata.get("source")
            page = chunk.metadata.get("page")
            current_page_id = f"{source}:{page}"

            # If the page ID is the same as the last one, increment the index.
            if current_page_id == last_page_id:
                current_chunk_index += 1
            else:
                current_chunk_index = 0

            # Calculate the chunk ID.
            chunk_id = f"{current_page_id}:{current_chunk_index}"
            last_page_id = current_page_id

            # Add it to the page meta-data.
            chunk.metadata["id"] = chunk_id

        return chunks

    # æ·»åŠ æ–‡ä»¶è‡³chroma
    def add_to_chroma(self, chunks: list[Document]):
        # Load the existing database.

        # Calculate Page IDs.
        chunks_with_ids = self.calculate_chunk_ids(chunks)

        # Add or Update the documents.
        existing_items = self.db.get(include=[])  # IDs are always included by default
        existing_ids = set(existing_items["ids"])
        print(f"Number of existing documents in DB: {len(existing_ids)}")

        # Only add documents that don't exist in the DB.
        new_chunks = []
        for chunk in chunks_with_ids:
            if chunk.metadata["id"] not in existing_ids:
                new_chunks.append(chunk)

        if len(new_chunks):
            print(f"ğŸ‘‰ Adding new documents: {len(new_chunks)}")
            new_chunk_ids = [chunk.metadata["id"] for chunk in new_chunks]
            self.db.add_documents(new_chunks, ids=new_chunk_ids)
            self.db.persist()
        else:
            print("âœ… No new documents to add")

    # å®ç°å¤šä¸ªæ–‡ä»¶çš„å¯¼å…¥å¹¶å­˜å‚¨åœ¨çŸ¥è¯†åº“ä¸­
    def add_folder(self, folder_path: str) -> str:
        for filename in os.listdir(folder_path):
            # æ„å»ºå®Œæ•´çš„æ–‡ä»¶è·¯å¾„
            if filename.startswith('.'):
                continue
            file_path = os.path.join(folder_path, filename)
            doc = base.import_file(file_path)
            if doc is not False:
                chunks = self.split_documents(doc)
                # åµŒå…¥å¹¶å­˜å…¥Chromaæ•°æ®åº“
                self.add_to_chroma(chunks)
            else:
                print(f"{file_path}æ–‡ä»¶ç±»å‹é”™è¯¯æˆ–å¯¼å…¥å¤±è´¥")
        return "æ‰€æœ‰æ–‡ä»¶å·²å…¨éƒ¨å¯¼å…¥çŸ¥è¯†åº“ï¼Œè¯·æ£€æŸ¥æ–‡ä»¶å†…å®¹"

    def delete_from_chroma(self, file_path):
        pass

    # æ¸…é™¤çŸ¥è¯†åº“å†…å®¹
    def clear_database(self):
        if os.path.exists(self.chroma_path):
            shutil.rmtree(self.chroma_path)

    # Emailé¢„å¤„ç†


# æ£€ç´¢çŸ¥è¯†åº“ç±»ï¼Œå…¶ä¸­åŒ…æ‹¬ï¼šï¼ˆç›¸å…³äººç‰©çš„ï¼‰æŸ¥è¯¢ã€çº¿ç´¢å‘ç°ã€åˆ†æã€æ€»ç»“ã€å†™ä½œ
# è®°å¾—æ·»åŠ è¯­éŸ³åˆ†æåŠŸèƒ½â€¼ï¸
class QueryKnowledgeBase():
    def __init__(self, chroma_path="image", file_num=5):
        # é»˜è®¤ç¼–ç æ¨¡å‹
        self.embeddings = OllamaEmbeddings(model="yxl/m3e")
        # é»˜è®¤çŸ¥è¯†åº“
        self.chroma_path = chroma_path
        if self.chroma_path == "default":
            self.db = Chroma(
                embedding_function=self.embeddings
            )
        else:
            self.db = Chroma(
                persist_directory=self.chroma_path, embedding_function=self.embeddings
            )
        # é»˜è®¤æ¨¡å‹ä¸ºqwen2:7b
        self.llm = Ollama(model="qwen2")
        # é»˜è®¤æ£€ç´¢ç‰‡æ®µæ•°
        self.file_num = file_num
        self.PROMPT_TEMPLATE = """
        æ ¹æ®ä»¥ä¸‹contextå›ç­”é—®é¢˜ï¼š{question}ï¼Œè¦æ±‚è¯¦ç»†ä½¿ç”¨å¹¶ä¸”è¯¦ç»†æŒ‡å‡ºçŸ¥è¯†æ¥æºã€‚
        {context}
        """

    def query_context(self, query_text: str) -> str:
        # æ ¹æ®æŸ¥è¯¢è·å–ç›¸å…³å†…å®¹
        # æœç´¢å‘é‡æ•°æ®åº“
        results = self.db.similarity_search_with_score(query_text, k=self.file_num)
        print(results)
        context_text = "\n\n---\n\n".join([doc.page_content for doc, _score in results])
        # # è¾“å‡ºæŸ¥æ‰¾åˆ°çš„å†…å®¹
        # print(context_text)
        # print("-----------------------")
        return context_text

    def query_content(self, query_text: str) -> str:
        # æ ¹æ®æŸ¥è¯¢è·å–ç›¸å…³å†…å®¹
        # æœç´¢å‘é‡æ•°æ®åº“
        results = self.db.similarity_search_with_score(query_text, k=self.file_num)
        # print(results)
        context_text = "\n\n---\n\n".join([doc.page_content for doc, _score in results])
        # # è¾“å‡ºæŸ¥æ‰¾åˆ°çš„å†…å®¹
        # print(context_text)
        # print("-----------------------")
        prompt_template = ChatPromptTemplate.from_template(self.PROMPT_TEMPLATE)
        prompt = prompt_template.format(context=context_text, question=query_text)
        # model = Ollama(model="qwen2")
        response_text = self.llm.invoke(prompt)

        sources = [doc.metadata.get("id", None) for doc, _score in results]
        formatted_response = f"{response_text}\n\næ–‡ä»¶æ¥æºå¦‚ä¸‹: {sources}"
        # print(formatted_response)
        return formatted_response


# æ–‡ä»¶ç±»ï¼Œå…¶ä¸­åŒ…æ‹¬ï¼šï¼ˆå¤šæ–‡ä»¶ï¼‰æ€»ç»“ä¸åˆ†æã€ï¼ˆä¿ç•™æ ¼å¼çš„é«˜ç²¾åº¦ï¼‰ç¿»è¯‘ã€ï¼ˆé«˜è´¨é‡ã€ç»Ÿä¸€æ ¼å¼ï¼‰å†™ä½œ
class QueryFileBase(QueryKnowledgeBase):
    def __init__(self):
        super().__init__()

    def translate_content(self, file_path: str, source_lang, target_lang):
        # å®ç°ç¿»è¯‘é€»è¾‘
        # è·å–æ–‡ä»¶ä¸­çš„æ–‡å­—
        doc = import_file(file_path)
        # ç¡®ä¿æ ¼å¼æ­£ç¡®
        if doc is not False:
            source_text = doc[0].page_content
            print(source_text)
            translation = utils.translate(
                source_lang=source_lang,
                target_lang=target_lang,
                source_text=source_text
            )
            return translation
        else:
            return f"{file_path}ç±»å‹é”™è¯¯æˆ–å¯¼å…¥å¤±è´¥"

    def conclusion(self, folder_path: str):
        conclusion_prompt = ChatPromptTemplate.from_messages(
            [("system", "ç®€è¦æ¦‚æ‹¬ä»¥ä¸‹å†…å®¹ï¼Œè¦æ±‚å†…å®¹æ–‡å­—è¿è´¯ï¼Œå¹¶æ‰¾åˆ°å…¶ä¸­çš„å…³è”æ€§:\\n\\n{context}ã€‚ä»¥ä¸­æ–‡è¾“å‡ºä½ æ¦‚æ‹¬çš„å†…å®¹ã€‚")]
        )
        context = []
        for file in os.listdir(folder_path):
            file_path = os.path.join(folder_path, file)
            docs = import_file(file_path)
            context.append(docs[0])

        chain = create_stuff_documents_chain(self.llm, conclusion_prompt)
        # Invoke chain
        result = chain.invoke({"context": context})
        return result


# å›¾è°±ç±»
class GraphBase(QueryFileBase):
    def __init__(self):
        super().__init__()
        os.environ["NEO4J_URI"] = "bolt://localhost:7687"
        os.environ["NEO4J_USERNAME"] = "neo4j"
        os.environ["NEO4J_PASSWORD"] = "lkzxxzcsc2020"
        os.environ["NEO4J_DATABASE"] = "neo4j"

        self.graph = Neo4jGraph()
        # è¿æ¥åˆ°Neo4jæ•°æ®åº“
        self.graph_db = Graph(os.getenv("NEO4J_URI"), auth=(os.getenv("NEO4J_USERNAME"), os.getenv("NEO4J_PASSWORD")))

    def build_knowledge_graph(self, folder_path: str):
        doc = []
        for file in os.listdir(folder_path):
            file_path = os.path.join(folder_path, file)
            docs = import_file(file_path)
            if docs is not False:
                doc.append(docs[0])
        print(doc)
        llm_transformer = LLMGraphTransformer(
            llm=self.llm
        )
        graph_documents = llm_transformer.convert_to_graph_documents(documents=doc)
        print(f"Nodes:{graph_documents[0].nodes}")
        print(f"Relationships:{graph_documents[0].relationships}")

        try:
            self.graph.add_graph_documents(graph_documents)
            return True
        except Exception:
            return False

    def query_knowleage_graph(self, query):
        chain = GraphCypherQAChain.from_llm(
            self.llm,
            graph=self.graph,
            verbose=True
        )
        result = chain.invoke({"query": query})
        final_answer = result['result']
        return final_answer

    def delete_knowleage_graph(self):
        try:
            # æ¸…ç©ºNeo4jæ•°æ®åº“
            self.graph_db.run("MATCH (n) DETACH DELETE n")
            return True
        except Exception:
            return False



# basement = QueryKnowledgeBase()
# result = basement.query_content("è²å¾‹å®¾ç›¸å…³äº‹ä»¶")
# print(result)

